import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import random
import os
import sys
from torch.utils.tensorboard import SummaryWriter

current_file = os.path.abspath(__file__) 
project_root = os.path.abspath(os.path.join(current_file, "..", "..")) # í˜„ì¬ ë””ë ‰í† ë¦¬ì— ë”°ë¼ ì´ ë¶€ë¶„ ìˆ˜ì •
sys.path.append(project_root)

from manage import PathManager
path_manager = PathManager()

# ì›í•˜ëŠ” ê²½ë¡œ ì¶”ê°€
sys.path.append(path_manager.get_path("config"))
sys.path.append(path_manager.get_path("logs"))

try:
    from logs import log_manager
    from config import config_manager
except Exception as e:
    print(f"ì„í¬íŠ¸ ì‹¤íŒ¨: {e}")

# ëª¨ë¸ê³¼ ëª¨ë“  í…ì„œë¥¼ deviceë¡œ ë³€í™˜í•˜ëŠ” .to(self.device)ë¥¼ ëª¨ë“  ì—°ì‚°ì—ì„œ ëª…ì‹œì ìœ¼ë¡œ ì ìš©í•´ì•¼ í•¨

class PPOAgent:
    """PPO(Proximal Policy Optimization) ì—ì´ì „íŠ¸"""

    def __init__(self, model, writer=None):
        self.device = torch.device(config_manager.get_device())  # device ì„¤ì •
        self.model = model.to(self.device)  # ëª¨ë¸ì„ GPU/CPUë¡œ ì´ë™
        self.optimizer = optim.Adam(self.model.parameters(), lr=config_manager.get_learning_rate()) # Adam Optimizer ì„¤ì •
        self.gamma = config_manager.get_gamma() # í• ì¸ìœ¨(Î³)
        self.clampepsilon = config_manager.get_clampepsilon() # PPO í´ë¦¬í•‘ íŒŒë¼ë¯¸í„°(Îµ)
        self.batch_size = config_manager.get_batch_size() # ë°°ì¹˜ í¬ê¸°
        self.criterion = nn.MSELoss() # ì†ì‹¤ í•¨ìˆ˜ ì„¤ì •
        self.epsilon = config_manager.get_epsilon()
        self.epsilon_min = config_manager.get_epsilon_min()
        self.epsilon_decay = config_manager.get_epsilon_decay()
        self.max_shares_per_trade = config_manager.get_max_shares_per_trade()
        self.action_dim = 1 + 2 * self.max_shares_per_trade
        self.temperature = 3.0
        # temperature > 1 â†’ ë¶„í¬ë¥¼ í‰í‰í•˜ê²Œ (ë” ë§ì€ íƒí—˜)
        # temperature < 1 â†’ ë¶„í¬ë¥¼ ë” ë‚ ì¹´ë¡­ê²Œ (ê²°ì •ì  í–‰ë™ ê°•í™”)
        self.entropy_coef = 0.02  # âœ… ì¡°ì • ê°€ëŠ¥

        # âœ… TensorBoard ì„¤ì •
        self.writer = writer 
        self.train_step = 0  # í•™ìŠµ ìŠ¤í… ì¹´ìš´íŠ¸

    def select_action(self, state):
        """í˜„ì¬ ìƒíƒœì—ì„œ í™•ë¥ ì ìœ¼ë¡œ ì•¡ì…˜ì„ ì„ íƒ"""
        state = torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(self.device)  # (1, seq_len, feature_dim+1) â† shares_held ì¶”ê°€

        if not torch.isfinite(state).all():
            print("âš ï¸ Invalid state detected:", state)
        
        # logitsëŠ” ëª¨ë¸ì˜ ë§ˆì§€ë§‰ ì¶œë ¥ì¸µì—ì„œ ë‚˜ì˜¨ ê°€ê³µë˜ì§€ ì•Šì€ ê°’ë“¤
        logits = self.model(state)  # ëª¨ë¸ì˜ ì›ì‹œ ì¶œë ¥

        # logits ì¶œë ¥ ê²€ì¦ ë° ë””ë²„ê¹…ìš© ë¡œê·¸ (1D í…ì„œë¡œ ë³€í™˜)
        logits_list = logits[0].tolist()

        # 100 stepë§ˆë‹¤ ì¼ë¶€ ì•¡ì…˜ logits ì¶œë ¥
        if self.train_step % 100 == 0:
            # ìƒìœ„ 10ê°œ logits ê°’ê³¼ ì¸ë±ìŠ¤ ì¶”ì¶œ
            topk = sorted(enumerate(logits_list), key=lambda x: x[1], reverse=True)[:3]

            topk_log = {}
            for idx, val in topk:
                if idx == 0:
                    action_type = "ê´€ë§"
                    action_info = f"{action_type}"
                elif 1 <= idx <= self.max_shares_per_trade:
                    action_type = "ë§¤ìˆ˜"
                    shares = idx  # ë§¤ìˆ˜ ìˆ˜ëŸ‰
                    action_info = f"{action_type}({shares}ì£¼)"
                else:
                    action_type = "ë§¤ë„"
                    shares = idx - self.max_shares_per_trade  # ë§¤ë„ ìˆ˜ëŸ‰
                    action_info = f"{action_type}({shares}ì£¼)"

                topk_log[f"Action_{idx}"] = f"{val:.4f} â†’ {action_info}"

            log_manager.logger.debug(
                f"{self.train_step} step Top-10 Raw logits:\n{topk_log}"
            )

        # ğŸ” ëª¨ë¸ ì¶œë ¥(logits)ì˜ ìœ íš¨ì„± ê²€ì‚¬
        if not torch.isfinite(logits).all():
            print("âš ï¸ Invalid logits detected:", logits)

        # probability(í™•ë¥ )
        probs = torch.softmax(logits / self.temperature, dim=-1) # í˜„ì¬ ìƒíƒœ(state)ë¥¼ StockTransformer ëª¨ë¸ì— ì…ë ¥, probs = í™•ë¥  ë¶„í¬ Ï€Î¸(a|s)
        dist = torch.distributions.Categorical(probs)

        if random.random() < self.epsilon:
            action = random.choice(list(range(self.action_dim)))
            log_prob = dist.log_prob(torch.tensor(action).to(self.device))  # âœ… ì‹ ê²½ë§ ê¸°ë°˜ log_prob
            # action_names = ["ë§¤ë„", "ê´€ë§", "ë§¤ìˆ˜"]
            # log_manager.logger.debug(f"[íƒí—˜] ëœë¤ ì•¡ì…˜ ì„ íƒ: {action} ({action_names[action]}) (ì…ì‹¤ë¡ ={self.epsilon:.4f})")
        else:
            action = dist.sample().item()
            log_prob = dist.log_prob(torch.tensor(action).to(self.device))

        # log_manager.logger.debug(f"action: {action}, log_prob: {log_prob.item():.4f}")
        self.epsilon = max(self.epsilon * self.epsilon_decay, self.epsilon_min)

        # print(f"ğŸ“ Logging at step {self.train_step}: Sell={probs[0,0].item():.4f}, Hold={probs[0,1].item():.4f}, Buy={probs[0,2].item():.4f}")

        self.train_step += 1  # í•™ìŠµ ìŠ¤í… ì¦ê°€
        # âœ… TensorBoardì— action í™•ë¥  ê¸°ë¡
        if self.action_dim <= 10:
            for i in range(self.action_dim):
                self.writer.add_scalar(f"Action_Prob/Action_{i}", probs[0, i].item(), self.train_step)


        # âš ï¸ í™•ë¥  ê°’ì˜ ìœ íš¨ì„± ê²€ì‚¬ë§Œ ì§„í–‰ (í´ë¦¬í•‘ X)
        if not torch.isfinite(probs).all() or (probs < 0).any():
            print("âš ï¸ Invalid probability tensor detected:", probs)
            return random.choice([0, 1, 2])  # ë¬¸ì œê°€ ë°œìƒí•˜ë©´ ëœë¤ ì•¡ì…˜ ë°˜í™˜

        return action, log_prob.item()

    def update(self, memory):
        """PPO ì•Œê³ ë¦¬ì¦˜ì„ ì´ìš©í•œ ì •ì±… ì—…ë°ì´íŠ¸"""
        states, actions, rewards, old_log_probs = zip(*memory)
        states = torch.tensor(np.array(states), dtype=torch.float32).to(self.device)
        actions = torch.tensor(actions, dtype=torch.int64).to(self.device)
        rewards = torch.tensor(rewards, dtype=torch.float32).to(self.device)
        old_log_probs = torch.tensor(old_log_probs, dtype=torch.float32).to(self.device)

        # âœ… 1. Discounted Reward ê³„ì‚° (Advantage Estimation)
        discounted_rewards = [] # Advantage Estimation
        sum_reward = 0
        for r in reversed(rewards):
            sum_reward = r + self.gamma * sum_reward # gamma(Î³) ê°’(í• ì¸ìœ¨)ì„ ì‚¬ìš©í•˜ì—¬ ë¯¸ë˜ ë³´ìƒì„ í˜„ì¬ ê°€ì¹˜ë¡œ ë³€í™˜
            discounted_rewards.insert(0, sum_reward)
        discounted_rewards = torch.tensor(discounted_rewards, dtype=torch.float32).to(self.device)

        for i in range(0, len(states), self.batch_size):
            batch_states = states[i:i+self.batch_size]
            batch_actions = actions[i:i+self.batch_size]
            batch_rewards = discounted_rewards[i:i+self.batch_size]
            batch_old_log_probs = old_log_probs[i:i+self.batch_size]

            # âœ… 2. ìƒˆë¡œìš´ ì •ì±…(`Ï€_new`)ì˜ í™•ë¥  ê³„ì‚°
            probs = torch.softmax(self.model(batch_states) / self.temperature, dim=-1)
            # action_probs = probs.gather(1, batch_actions.unsqueeze(1)).squeeze()
            dist = torch.distributions.Categorical(probs)
            new_log_probs = dist.log_prob(batch_actions)

            # âœ… 3. PPO Clipped Objective ê³„ì‚°
            # PPO Clipped Objective ê³„ì‚°
            ratio = torch.exp(new_log_probs - batch_old_log_probs) # í™•ë¥  ë¹„ìœ¨(`Ï€_new / Ï€_old`) ê³„ì‚°

            # ğŸ” í™•ë¥  ë¹„ìœ¨ì´ ë„ˆë¬´ í¬ê±°ë‚˜ ì‘ì€ ê²½ìš° í™•ì¸
            if (ratio > 10).any() or (ratio < 0.1).any():
                print(f"âš ï¸ ì´ìƒí•œ ratio ê°’ ê°ì§€! min: {ratio.min().item()}, max: {ratio.max().item()}")

            # ratioì˜ ìœ íš¨ì„± ê²€ì‚¬
            if not torch.isfinite(ratio).all():
                print("âš ï¸ Invalid ratio detected:", ratio)
            
            entropy = dist.entropy().mean()

            clipped_ratio = torch.clamp(ratio, 1 - self.clampepsilon, 1 + self.clampepsilon) # í™•ë¥  ë¹„ìœ¨ì´ ë„ˆë¬´ ì»¤ì§€ì§€ ì•Šë„ë¡ í´ë¦¬í•‘(Îµ=0.2) ì ìš©
            loss = -torch.min(ratio * batch_rewards, clipped_ratio * batch_rewards).mean() # ì†ì‹¤ í•¨ìˆ˜
            loss -= self.entropy_coef * entropy  # âœ… ì—”íŠ¸ë¡œí”¼ ë³´ìƒ ì¶”ê°€

            # âœ… TensorBoard ê¸°ë¡ ì¶”ê°€
            if self.writer:
                self.writer.add_scalar("Loss", loss.item(), self.train_step)
                self.writer.add_scalar("PPO Ratio Mean", ratio.mean().item(), self.train_step)  # í™•ë¥  ë¹„ìœ¨ ê¸°ë¡
                self.writer.add_scalar("PPO Clipped Ratio Mean", clipped_ratio.mean().item(), self.train_step)  # í´ë¦¬í•‘ ë¹„ìœ¨ ê¸°ë¡
                self.writer.add_scalar("Batch Reward Mean", batch_rewards.mean().item(), self.train_step)  # ë³´ìƒ í‰ê·  ê¸°ë¡

            # âœ… 4. ëª¨ë¸ ì—…ë°ì´íŠ¸
            self.optimizer.zero_grad()
            loss.backward() # PPO ì†ì‹¤ì„ ì—­ì „íŒŒ(Backpropagation)í•˜ì—¬ ëª¨ë¸ì˜ ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸

            # ê° íŒŒë¼ë¯¸í„°ì˜ ê¸°ìš¸ê¸°(Gradient) ê°’ì„ í™•ì¸(ê¸°ìš¸ê¸° í­ë°œ ê°’)
            for name, param in self.model.named_parameters():
                if param.grad is not None and not torch.isfinite(param.grad).all():
                    print(f"âš ï¸ Invalid gradient detected in {name}: {param.grad}")
                    return  # í•™ìŠµì„ ì¤‘ë‹¨í•˜ê³  ë””ë²„ê¹…ì„ ì§„í–‰

            self.optimizer.step() # Adam Optimizerë¥¼ ì‚¬ìš©í•˜ì—¬ ê°€ì¤‘ì¹˜ ì¡°ì •
            self.train_step += 1  # í•™ìŠµ ë‹¨ê³„ ì¦ê°€

if __name__ == "__main__":
    from models.transformer_model import StockTransformer
    # âœ… ì„¤ì • ê°€ì ¸ì˜¤ê¸°
    device = torch.device(config_manager.get_device())
    input_dim = config_manager.get_input_dim()
    seq_len = 30  # ì‹œê³„ì—´ ê¸¸ì´

    # âœ… ê°€ì§œ ë°ì´í„°(Mock Data) ìƒì„± (ëœë¤ ë°ì´í„°)
    batch_size = config_manager.get_batch_size()
    test_states = torch.randn(batch_size, seq_len, input_dim+1).to(device)  # (batch, seq_len, feature_dim)
    
    # âœ… Transformer ëª¨ë¸ ìƒì„± ë° PPOAgent ì´ˆê¸°í™”
    model = StockTransformer(input_dim=input_dim).to(device)
    agent = PPOAgent(model)

    # âœ… ì•¡ì…˜ ì„ íƒ í…ŒìŠ¤íŠ¸
    log_manager.logger.debug("\nğŸ¯ ì•¡ì…˜ ì„ íƒ í…ŒìŠ¤íŠ¸:")
    test_state = test_states[0].to(device).numpy()  # (seq_len, input_dim)
    action = agent.select_action(test_state)
    log_manager.logger.debug(f"ğŸ”¹ ì„ íƒëœ ì•¡ì…˜: {action}")  # 0 (ë§¤ë„), 1 (ë³´ìœ ), 2 (ë§¤ìˆ˜)
    
    # âœ… ì—…ë°ì´íŠ¸ í…ŒìŠ¤íŠ¸ (ê°€ì§œ ë©”ëª¨ë¦¬ ë°ì´í„°)
    log_manager.logger.debug("\nğŸ“Œ ì—ì´ì „íŠ¸ ì—…ë°ì´íŠ¸ í…ŒìŠ¤íŠ¸:")
    action_dim = 1 + 2 * config_manager.get_max_shares_per_trade()

    test_memory = []
    for i in range(batch_size):
        state = test_states[i].cpu().numpy()
        action, log_prob = agent.select_action(state)  # âœ… log_prob í¬í•¨
        reward = np.random.randn()
        test_memory.append((state, action, reward, log_prob))

    log_manager.logger.debug(f"ğŸ“ ë©”ëª¨ë¦¬ ìƒ˜í”Œ ê°œìˆ˜: {len(test_memory)}")
    agent.update(test_memory)
    log_manager.logger.debug("âœ… ì—ì´ì „íŠ¸ ì—…ë°ì´íŠ¸ ì™„ë£Œ!")

