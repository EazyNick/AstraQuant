import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np

import os
import sys

current_file = os.path.abspath(__file__) 
project_root = os.path.abspath(os.path.join(current_file, "..", "..")) # í˜„ì¬ ë””ë ‰í† ë¦¬ì— ë”°ë¼ ì´ ë¶€ë¶„ ìˆ˜ì •
sys.path.append(project_root)

from manage import PathManager
path_manager = PathManager()

# ì›í•˜ëŠ” ê²½ë¡œ ì¶”ê°€
sys.path.append(path_manager.get_path("config"))
sys.path.append(path_manager.get_path("logs"))

# import
try:
    from logs import log_manager
    from config import config_manager
except Exception as e:
    print(f"ì„í¬íŠ¸ ì‹¤íŒ¨: {e}")

# ëª¨ë¸ê³¼ ëª¨ë“  í…ì„œë¥¼ deviceë¡œ ë³€í™˜í•˜ëŠ” .to(self.device)ë¥¼ ëª¨ë“  ì—°ì‚°ì—ì„œ ëª…ì‹œì ìœ¼ë¡œ ì ìš©í•´ì•¼ í•¨

class PPOAgent:
    def __init__(self, model):
        self.device = torch.device(config_manager.get_device())  # âœ… device ì„¤ì •

        self.model = model.to(self.device)  # âœ… ëª¨ë¸ì„ GPU/CPUë¡œ ì´ë™
        self.optimizer = optim.Adam(self.model.parameters(), lr=config_manager.get_learning_rate())
        self.gamma = config_manager.get_gamma()
        self.epsilon = config_manager.get_epsilon()
        self.batch_size = config_manager.get_batch_size()
        self.criterion = nn.MSELoss()

    def select_action(self, state):
        state = torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(self.device)  # (1, seq_len, feature_dim)
        probs = torch.softmax(self.model(state), dim=-1)
        action = torch.multinomial(probs, 1).item()
        return action

    def update(self, memory):
        states, actions, rewards = zip(*memory)
        states = torch.tensor(np.array(states), dtype=torch.float32).to(self.device)
        actions = torch.tensor(actions, dtype=torch.int64).to(self.device)
        rewards = torch.tensor(rewards, dtype=torch.float32).to(self.device)

        discounted_rewards = []
        sum_reward = 0
        for r in reversed(rewards):
            sum_reward = r + self.gamma * sum_reward
            discounted_rewards.insert(0, sum_reward)
        discounted_rewards = torch.tensor(discounted_rewards, dtype=torch.float32).to(self.device)

        for i in range(0, len(states), self.batch_size):
            batch_states = states[i:i+self.batch_size]
            batch_actions = actions[i:i+self.batch_size]
            batch_rewards = discounted_rewards[i:i+self.batch_size]

            probs = torch.softmax(self.model(batch_states), dim=-1)
            action_probs = probs.gather(1, batch_actions.unsqueeze(1)).squeeze()
            old_probs = action_probs.detach()

            ratio = action_probs / old_probs
            clipped_ratio = torch.clamp(ratio, 1 - self.epsilon, 1 + self.epsilon)
            loss = -torch.min(ratio * batch_rewards, clipped_ratio * batch_rewards).mean()

            self.optimizer.zero_grad()
            loss.backward()
            self.optimizer.step()

if __name__ == "__main__":
    from models.transformer_model import StockTransformer  # ëª¨ë¸ ì„í¬íŠ¸
    # âœ… ì„¤ì • ê°€ì ¸ì˜¤ê¸°
    device = torch.device(config_manager.get_device())
    input_dim = config_manager.get_input_dim()
    seq_len = 30  # ì‹œê³„ì—´ ê¸¸ì´

    # âœ… ê°€ì§œ ë°ì´í„°(Mock Data) ìƒì„± (ëœë¤ ë°ì´í„°)
    batch_size = config_manager.get_batch_size()
    test_states = torch.randn(batch_size, seq_len, input_dim).to(device)  # (batch, seq_len, feature_dim)
    
    # âœ… Transformer ëª¨ë¸ ìƒì„± ë° PPOAgent ì´ˆê¸°í™”
    model = StockTransformer(input_dim=input_dim).to(device)
    agent = PPOAgent(model)

    # âœ… ì•¡ì…˜ ì„ íƒ í…ŒìŠ¤íŠ¸
    print("\nğŸ¯ ì•¡ì…˜ ì„ íƒ í…ŒìŠ¤íŠ¸:")
    test_state = test_states[0].cpu().numpy()  # ë‹¨ì¼ ìƒ˜í”Œ (CPUë¡œ ë³€í™˜í•˜ì—¬ í…ŒìŠ¤íŠ¸)
    action = agent.select_action(test_state)
    print(f"ğŸ”¹ ì„ íƒëœ ì•¡ì…˜: {action}")  # 0 (ë§¤ë„), 1 (ë³´ìœ ), 2 (ë§¤ìˆ˜)

    # âœ… ì—…ë°ì´íŠ¸ í…ŒìŠ¤íŠ¸ (ê°€ì§œ ë©”ëª¨ë¦¬ ë°ì´í„°)
    print("\nğŸ“Œ ì—ì´ì „íŠ¸ ì—…ë°ì´íŠ¸ í…ŒìŠ¤íŠ¸:")
    test_memory = [
        (test_states[i].cpu().numpy(), np.random.randint(0, 3), np.random.randn()) 
        for i in range(batch_size)
    ]  # (state, action, reward) ëœë¤ ë°ì´í„° ìƒì„±

    print(f"ğŸ“ ë©”ëª¨ë¦¬ ìƒ˜í”Œ ê°œìˆ˜: {len(test_memory)}")
    agent.update(test_memory)
    print("âœ… ì—ì´ì „íŠ¸ ì—…ë°ì´íŠ¸ ì™„ë£Œ!")

