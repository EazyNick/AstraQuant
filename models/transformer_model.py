"""
(1) ì…ë ¥ ë°ì´í„° (batch, seq_len, input_dim)
        â”‚
        â–¼
(2) Embedding (Linear ë³€í™˜ â†’ model_dim í¬ê¸°ë¡œ ë§ì¶¤)
        â”‚
        â–¼
(3) Positional Encoding ì¶”ê°€ (ì‹œê°„ ì •ë³´ ë°˜ì˜)
        â”‚
        â–¼
(4) Transformer ì¸ì½”ë” (TransformerEncoderê°€ `num_layers`ê°œ ìŒ“ì¸)
        â”‚
        â–¼
(5) ë§ˆì§€ë§‰ íƒ€ì„ìŠ¤í…ì˜ ì¶œë ¥ì„ ê°€ì ¸ì˜´ (x[:, -1, :])
        â”‚
        â–¼
(6) Fully Connected Layer (3ì°¨ì›: Buy, Hold, Sell)
        â”‚
        â–¼
(7) ìµœì¢… ì˜ˆì¸¡ ê²°ê³¼ (í™•ë¥  ê°’)
"""

import torch
import torch.nn as nn
from .positionalencoding import PositionalEncoding

import os
import sys

current_file = os.path.abspath(__file__) 
project_root = os.path.abspath(os.path.join(current_file, "..", "..")) # í˜„ì¬ ë””ë ‰í† ë¦¬ì— ë”°ë¼ ì´ ë¶€ë¶„ ìˆ˜ì •
sys.path.append(project_root)

from manage import PathManager
path_manager = PathManager()

# ì›í•˜ëŠ” ê²½ë¡œ ì¶”ê°€
sys.path.append(path_manager.get_path("config"))
sys.path.append(path_manager.get_path("logs"))

# import
try:
    from logs import log_manager
    from config import config_manager
except Exception as e:
    print(f"ì„í¬íŠ¸ ì‹¤íŒ¨: {e}")

class StockTransformer(nn.Module):
    def __init__(self, input_dim=None):
        super(StockTransformer, self).__init__()

        self.device = torch.device(config_manager.get_device())
        model_dim = config_manager.get_model_dim()
        num_heads = config_manager.get_num_heads()
        num_layers = config_manager.get_num_layers()

        # âœ… ì…ë ¥ ì°¨ì› ì„¤ì • (ê¸°ë³¸ê°’: configì—ì„œ ë¶ˆëŸ¬ì˜¤ê¸°)
        if input_dim is None:
            input_dim = config_manager.get_input_dim()

        self.positional_encoding = PositionalEncoding(model_dim).to(self.device)
        self.layer_norm = nn.LayerNorm(model_dim).to(self.device)  # ì¶”ê°€

        # ì…ë ¥ ë°ì´í„°ë¥¼ Transformer ì…ë ¥ ì°¨ì›ìœ¼ë¡œ ë³€í™˜
        self.embedding = nn.Linear(input_dim, model_dim).to(self.device)

        # Transformer ì¸ì½”ë” ë ˆì´ì–´ ì„¤ì • (batch_first=True ì˜µì…˜ ì¶”ê°€)
        encoder_layer = nn.TransformerEncoderLayer(d_model=model_dim, nhead=num_heads, batch_first=True).to(self.device)
        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers).to(self.device)

        # ë§¤ìˆ˜(Buy), ë³´ìœ (Hold), ë§¤ë„(Sell) 3ê°€ì§€ ì•¡ì…˜ì„ ìœ„í•œ ìµœì¢… FC ë ˆì´ì–´
        self.fc = nn.Linear(model_dim, 3).to(self.device)

    def forward(self, x):
        # log_manager.logger.debug(f"ì…ë ¥ ë°ì´í„° ì´ˆê¸° shape: {x.shape}")
        # xì˜ ì°¨ì›ì´ (batch, seq_len)ì´ë©´, ë§ˆì§€ë§‰ ì°¨ì›ì„ ì¶”ê°€í•´ (batch, seq_len, input_dim)ìœ¼ë¡œ ë³€í™˜
        if x.dim() == 2:  # (seq_len, feature_dim) â†’ batch ì°¨ì› ì—†ìŒ
            x = x.unsqueeze(0)  # (1, seq_len, feature_dim)
            log_manager.logger.debug(f"batch ì°¨ì› ì¶”ê°€ í›„ shape: {x.shape}")

        x = x.to(self.device)  # ì…ë ¥ ë°ì´í„° GPU ì´ë™
        x = self.embedding(x)  # (batch, seq_len, input_dim) -> (batch, seq_len, model_dim)
        # x = self.layer_norm(x)  # LayerNorm ì ìš©(TransformerEncoderLayerì—ì„œ ì´ë¯¸ 2ë²ˆ ì •ê·œí™” ì§„í–‰ë¨. ì…ë ¥ë°ì´í„° ì •ê·œí™”ê°€ í•„ìš”í•œ ê²½ìš° ì¶”ê°€)
        x = self.positional_encoding(x)  # ğŸ”¹ Positional Encoding(ìœ„ì¹˜ ì •ë³´) ì¶”ê°€
        x = self.transformer(x)  # Transformer ì¸ì½”ë”ë¥¼ í†µê³¼

        output = self.fc(x[:, -1, :])  # ë§ˆì§€ë§‰ íƒ€ì„ìŠ¤í…ì˜ ì¶œë ¥ì„ ì‚¬ìš©í•˜ì—¬ ë§¤ë§¤ ì‹ í˜¸ ì˜ˆì¸¡
        # log_manager.logger.debug(f"ìµœì¢… ì¶œë ¥ shape: {output.shape}")  # ìµœì¢… ì¶œë ¥ shape í™•ì¸

        return output

# âœ… StockTransformer ë‹¨ë… í…ŒìŠ¤íŠ¸
if __name__ == "__main__":
    model = StockTransformer()
    test_input = torch.randn(1, 30, config_manager.get_input_dim())  # (batch, seq_len, feature_dim)
    output = model(test_input)  # Transformer ëª¨ë¸ì„ ê±°ì¹œ í›„ ê²°ê³¼ ë°˜í™˜
    log_manager.logger.debug(f"âœ… ëª¨ë¸ ì¶œë ¥ í¬ê¸°: {output.shape}")  # âœ… (16, 3) â†’ (batch_size, action_classes), ê° ê°’ì€ Buy, Hold, Sell í™•ë¥ ì„ ì˜ë¯¸
    # log_manager.logger.debug("âœ… output:", output)
