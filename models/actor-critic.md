ğŸŸ© í™˜ê²½ (StockTradingEnv)
    â”‚
    â–¼
(1) í˜„ì¬ ìƒíƒœ(state): shape = (batch, seq_len, input_dim + 1)  â† ë³´ìœ  ì£¼ì‹ ìˆ˜ í¬í•¨
    â”‚
    â”œâ”€â”€â–¶ ğŸŸ  Actor ë„¤íŠ¸ì›Œí¬
    â”‚         â””â”€ (2) Embedding â†’ (3) Positional Encoding â†’ (4) Transformer ì¸ì½”ë”
    â”‚         â””â”€ (5) ë§ˆì§€ë§‰ íƒ€ì„ìŠ¤í… ì¶”ì¶œ (x[:, -1, :])
    â”‚         â””â”€ (6) FC â†’ (7) ì•¡ì…˜ logits â†’ softmax â†’ í™•ë¥ ë¶„í¬ Ï€(a|s)
    â”‚              â†“
    â”‚         ìƒ˜í”Œë§ or Îµ-greedy â†’ ì„ íƒëœ action, log_prob
    â”‚
    â””â”€â”€â–¶ ğŸ”µ Critic ë„¤íŠ¸ì›Œí¬
              â””â”€ ë™ì¼ êµ¬ì¡° (Embedding â†’ PE â†’ Transformer)
              â””â”€ (6) FC â†’ (7) ìƒíƒœ ê°€ì¹˜ V(s)
    
          â†“
ğŸ“¦ í™˜ê²½ì— action ì „ë‹¬ (env.step(action))
    â””â”€ next_state, reward, done ë°˜í™˜

          â†“
ğŸ§  ë©”ëª¨ë¦¬ì— (state, action, reward, log_prob, value) ì €ì¥

          â†“
ğŸ’¡ ì¼ì • stepë§ˆë‹¤ ì—…ë°ì´íŠ¸ ìˆ˜í–‰:
    â”œâ”€ (1) rewardë¡œ discounted return ê³„ì‚°
    â”œâ”€ (2) advantage = return - value
    â”œâ”€ (3) actor_loss = PPO objective (ratio, clipped)
    â”œâ”€ (4) critic_loss = MSE(value_pred, return)
    â””â”€ (5) ë‘ ë„¤íŠ¸ì›Œí¬ ëª¨ë‘ gradient descentë¡œ í•™ìŠµ

ğŸ“ˆ TensorBoard ê¸°ë¡: í™•ë¥ , ë¡œìŠ¤, advantage, return

"""
(1) ì…ë ¥ ë°ì´í„° (batch, seq_len, input_dim)
        â”‚
        â–¼
(2) Embedding (Linear ë³€í™˜ â†’ model_dim í¬ê¸°ë¡œ ë§ì¶¤)
        â”‚
        â–¼
(3) Positional Encoding ì¶”ê°€ (ì‹œê°„ ì •ë³´ ë°˜ì˜)
        â”‚
        â–¼
(4) Transformer ì¸ì½”ë” (TransformerEncoderê°€ `num_layers`ê°œ ìŒ“ì¸)
        â”‚
        â–¼
(5) ë§ˆì§€ë§‰ íƒ€ì„ìŠ¤í…ì˜ ì¶œë ¥ì„ ê°€ì ¸ì˜´ (x[:, -1, :])
        â”‚
        â–¼
(6) Fully Connected Layer (3ì°¨ì›: Buy, Hold, Sell)
        â”‚
        â–¼
(7) ìµœì¢… ì˜ˆì¸¡ ê²°ê³¼ (í™•ë¥  ê°’)
"""