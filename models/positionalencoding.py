"""ì‹œí€€ìŠ¤ì˜ ìˆœì„œë¥¼ ë„¤íŠ¸ì›Œí¬ê°€ ì¸ì‹í•  ìˆ˜ ìˆë„ë¡, ìœ„ì¹˜ ì •ë³´ë¥¼ ë²¡í„°ì— ë”í•´ì£¼ëŠ” ì—­í• """

import torch
import torch.nn as nn
import math

class PositionalEncoding(nn.Module):
    def __init__(self, d_model, dropout=0.1, max_len=5000):
        super(PositionalEncoding, self).__init__()
        self.dropout = nn.Dropout(p=dropout)

        # ğŸ”¹ Positional Encoding í–‰ë ¬ ì´ˆê¸°í™”
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)  # (max_len, 1)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))

        # ğŸ”¹ ì§ìˆ˜ ì¸ë±ìŠ¤ (2i)ì—ëŠ” sin í•¨ìˆ˜ ì ìš©
        pe[:, 0::2] = torch.sin(position * div_term)

        # ğŸ”¹ í™€ìˆ˜ ì¸ë±ìŠ¤ (2i+1)ì—ëŠ” cos í•¨ìˆ˜ ì ìš©
        pe[:, 1::2] = torch.cos(position * div_term)

        pe = pe.unsqueeze(0)  # ë°°ì¹˜ ì°¨ì› ì¶”ê°€ (1, max_len, d_model)

        # ğŸ”¹ register_buffer: í•™ìŠµë˜ì§€ ì•ŠëŠ” ìƒíƒœê°’ìœ¼ë¡œ ë“±ë¡ (GPUì—ì„œë„ ì‚¬ìš© ê°€ëŠ¥)
        self.register_buffer('pe', pe)

    def forward(self, x):
        """
        x: (batch_size, seq_len, d_model)
        """
        x = x + self.pe[:, :x.size(1), :].to(x.device)  # Positional Encoding ì¶”ê°€
        return self.dropout(x)
