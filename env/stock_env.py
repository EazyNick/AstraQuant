# ì£¼ì‹ ê±°ë˜ í™˜ê²½ì„ ì •ì˜í•˜ëŠ” í´ë˜ìŠ¤

import numpy as np
import gym
from gym import spaces
import random
import torch

import os
import sys

current_file = os.path.abspath(__file__) 
project_root = os.path.abspath(os.path.join(current_file, "..", "..")) # í˜„ì¬ ë””ë ‰í† ë¦¬ì— ë”°ë¼ ì´ ë¶€ë¶„ ìˆ˜ì •
sys.path.append(project_root)

from manage import PathManager
path_manager = PathManager()

# ì›í•˜ëŠ” ê²½ë¡œ ì¶”ê°€
sys.path.append(path_manager.get_path("config"))
sys.path.append(path_manager.get_path("logs"))

# import
try:
    from logs import log_manager
    from config import config_manager
except Exception as e:
    print(f"ì„í¬íŠ¸ ì‹¤íŒ¨: {e}")

class StockTradingEnv(gym.Env):
    def __init__(self, stock_data):
        super(StockTradingEnv, self).__init__()
        self.device = config_manager.get_device()
        self.initial_balance = config_manager.get_initial_balance()
        self.observation_window = config_manager.get_observation_window()
        self.transaction_fee = config_manager.get_transaction_fee() 
        self.feature_dim = stock_data.shape[1] # ì…ë ¥ ë°ì´í„°ì˜ feature ê°œìˆ˜ ìë™ ì„¤ì •
        self.stock_data = stock_data
        self.current_step = 0
        self.balance = self.initial_balance
        self.shares_held = 0 # ë³´ìœ  ì£¼ì‹ ìˆ˜
        self.previous_portfolio_value = self.initial_balance 
        
        self.action_space = spaces.Discrete(3)  # 0: ë§¤ë„, 1: ë³´ìœ , 2: ë§¤ìˆ˜
        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(self.observation_window, self.feature_dim), dtype=np.float32)

    def normalize_reward(self, value, scale=50000): # -1,000 ~ 1,000
        value = torch.tensor(value, dtype=torch.float32).to(self.device)  # Tensor ë³€í™˜ í›„ GPU/CPU ì´ë™
        return torch.tanh(value / scale) * scale  # ì •ê·œí™” ì ìš©

    def reset(self):
        """ í™˜ê²½ì„ ì´ˆê¸°í™”í•˜ê³  ì´ˆê¸° ìƒíƒœë¥¼ ë°˜í™˜ """
        self.current_step = 0
        self.balance = self.initial_balance
        self.shares_held = 0
        self.previous_portfolio_value = self.initial_balance 
        return self.stock_data[self.current_step:self.current_step + self.observation_window] # (observation_window ê°’, feature_dim) í¬ê¸°ì˜ ë°°ì—´ ë°˜í™˜

    def step(self, action):
        """ ì•¡ì…˜ì„ ì‹¤í–‰í•˜ê³  ìƒˆë¡œìš´ ìƒíƒœ, ë³´ìƒ, ì¢…ë£Œ ì—¬ë¶€ ë°˜í™˜ """
        reward = 0
        price = self.stock_data[self.current_step, 0]

        if action == 2:  # ë§¤ìˆ˜ (Buy)
            shares_to_buy = self.balance / (price * (1 + self.transaction_fee)) # ì‚´ ìˆ˜ ìˆëŠ” ìµœëŒ€ ì£¼ì‹ ìˆ˜
            shares_to_buy = int(shares_to_buy) # ì •ìˆ˜ ê°’ìœ¼ë¡œ ë³€í™˜ (ì†Œìˆ˜ì  ì´í•˜ ë²„ë¦¼)
            cost = shares_to_buy * price * (1 + self.transaction_fee)  # ê±°ë˜ ìˆ˜ìˆ˜ë£Œ í¬í•¨
            if cost <= self.balance:  # ì”ê³ ê°€ ì¶©ë¶„í•œ ê²½ìš°ì—ë§Œ ë§¤ìˆ˜
                self.shares_held += shares_to_buy
                self.balance -= cost
            else:
                reward -= 100000  # ë§¤ìˆ˜ë¥¼ ì›í–ˆì§€ë§Œ ì‹¤íŒ¨í•œ ê²½ìš° íŒ¨ë„í‹° ì¶”ê°€

        elif action == 0:
            if self.shares_held > 0: # ë§¤ë„ (Sell)
                revenue = self.shares_held * price * (1 - self.transaction_fee)  # ê±°ë˜ ìˆ˜ìˆ˜ë£Œ í¬í•¨
                self.balance += revenue
                self.shares_held = 0  # ì „ëŸ‰ ë§¤ë„
            else:
                reward -= -150000  # ë§¤ë„ë¥¼ ì›í–ˆì§€ë§Œ ì‹¤íŒ¨í•œ ê²½ìš° íŒ¨ë„í‹° ì¶”ê°€
        
        self.current_step += 1
        done = self.current_step >= len(self.stock_data) - self.observation_window
        next_state = self.stock_data[self.current_step:self.current_step + self.observation_window]

        # ìƒˆë¡œìš´ í¬íŠ¸í´ë¦¬ì˜¤ ê°€ì¹˜ ê³„ì‚°
        new_portfolio_value = self.balance + (self.shares_held * price)

        short_term_reward = 0
        long_term_reward = 0
        holding_reward = 0
        future_reward = 0

        # í¬íŠ¸í´ë¦¬ì˜¤ ê°€ì¹˜ ë³€í™”ìœ¨ì„ ë³´ìƒìœ¼ë¡œ ì„¤ì • (ìˆ˜ìµë¥  ê¸°ë°˜ ë³´ìƒ), ë‹¨ê¸° ìˆ˜ìµë¥  ë³´ìƒ
        if self.previous_portfolio_value > 0:
            short_term_reward = ((new_portfolio_value - self.previous_portfolio_value) / self.previous_portfolio_value) * 100 * 50
        else:
            short_term_reward = 0

        # ì¥ê¸°ì  ë³´ìƒì„ ë°˜ì˜í•˜ë„ë¡ ê°•í™” (í˜„ì¬ ê°€ì¹˜ ëŒ€ë¹„ ì´ˆê¸° ê°€ì¹˜)
        long_term_reward = ((new_portfolio_value - self.initial_balance) / self.initial_balance) * 100 * 5

        # ë³´ìœ  ì£¼ì‹ ê°€ê²© ìƒìŠ¹ ì‹œ ì¶”ê°€ ë³´ìƒ
        if self.shares_held > 0 and self.current_step > 0:
            holding_reward = (price - self.stock_data[self.current_step - 1, 0]) * self.shares_held * 2
        else:
            holding_reward = 0

        # 25ì¼ í›„ì˜ `Buy & Hold` ìˆ˜ìµë¥  ê³„ì‚°
        future_step = min(self.current_step + 25, len(self.stock_data) - 1)
        # í˜„ì¬ ìŠ¤í…ì„ ì œì™¸í•œ 25ì¼ ì´ë‚´ì˜ ìµœê³ ê°€ & ìµœì €ê°€ ì°¾ê¸°
        future_max_price = np.max(self.stock_data[self.current_step + 1:future_step + 1, 0])
        future_min_price = np.min(self.stock_data[self.current_step + 1:future_step + 1, 0])
        
        # ë¦¬ì›Œë“œ ê³„ì‚°
        if action == 2:  # ë§¤ìˆ˜(Buy)
            if future_max_price <= price:  # ë¯¸ë˜ ìµœê³ ê°€ê°€ í˜„ì¬ ê°€ê²©ë³´ë‹¤ ë‚®ê±°ë‚˜ ê°™ìœ¼ë©´ ì†ì‹¤ ê°€ëŠ¥ì„±ì´ í¼
                future_return = ((future_min_price - price) / price) * self.shares_held * 100
            else:  # ë¯¸ë˜ ìµœê³ ê°€ê°€ í˜„ì¬ ê°€ê²©ë³´ë‹¤ ë†’ìœ¼ë©´ ê¸°ì¡´ ë°©ì‹ ìœ ì§€
                future_return = ((future_max_price - price) / price) * self.shares_held * 100
        elif action == 0:  # ë§¤ë„(Sell)
            if future_min_price >= price:  # ë¯¸ë˜ ìµœì €ê°€ê°€ í˜„ì¬ ê°€ê²©ë³´ë‹¤ ë†’ê±°ë‚˜ ê°™ìœ¼ë©´ ì†ì‹¤ ê°€ëŠ¥ì„±ì´ í¼
                future_return = ((price - future_max_price) / price) * self.shares_held * 100
            else:  # ë¯¸ë˜ ìµœì €ê°€ê°€ í˜„ì¬ ê°€ê²©ë³´ë‹¤ ë‚®ìœ¼ë©´ ê¸°ì¡´ ë°©ì‹ ìœ ì§€
                future_return = ((price - future_min_price) / price) * self.shares_held * 100

        elif action == 1:  # ê´€ë§(Hold)
            if self.shares_held > 0:  # ì£¼ì‹ì„ ë³´ìœ  ì¤‘ì´ë¼ë©´
                # ë¯¸ë˜ ìµœê³ ê°€ì™€ í˜„ì¬ ê°€ê²© ë¹„êµ
                if future_max_price > price:  # ê°€ê²©ì´ ì˜¤ë¥¼ ê²½ìš° í° ë³´ìƒ
                    future_return = ((future_max_price - price) / price) * self.shares_held * 100  # ê°€ê²© ìƒìŠ¹ ë³´ìƒ
                else:  # ê°€ê²©ì´ ë–¨ì–´ì§€ê±°ë‚˜ ê·¸ëŒ€ë¡œì¸ ê²½ìš° íŒ¨ë„í‹°
                    future_return = ((future_min_price - price) / price) * self.shares_held * 200  # íŒ¨ë„í‹°ëŠ” ë” í¬ê²Œ (ìŒìˆ˜ê°’)
            else:  # ì£¼ì‹ì„ ë³´ìœ í•˜ì§€ ì•Šì€ ìƒíƒœë¼ë©´
                # ë¯¸ë˜ ê°€ê²©ì´ ì˜¤ë¥´ë©´ ì£¼ì‹ì„ ì‚¬ì§€ ì•Šì€ ê²ƒì— ëŒ€í•œ íŒ¨ë„í‹°
                if future_max_price > price:
                    future_return = -((future_max_price - price) / price) * 200  # ë§¤ìˆ˜ ê¸°íšŒë¥¼ ë†“ì¹œ ê²ƒì— ëŒ€í•œ íŒ¨ë„í‹°
                # ë¯¸ë˜ ê°€ê²©ì´ ë–¨ì–´ì§€ë©´ ì£¼ì‹ì„ ì‚¬ì§€ ì•Šì€ ê²ƒì— ëŒ€í•œ ë³´ìƒ
                else:
                    future_return = ((price - future_min_price) / price) * 100  # í•˜ë½ì„ í”¼í•œ ê²ƒì— ëŒ€í•œ ë³´ìƒ
                
        future_reward = future_return * 300  # ìˆ˜ìµë¥  ê¸°ë°˜ ë³´ìƒ

        # âœ… ìµœì¢… ë³´ìƒ (ê° ë³´ìƒ ìš”ì†Œë¥¼ í•©ì‚°)
        reward = short_term_reward + long_term_reward + holding_reward + future_reward + reward

        # âœ… ë³´ìƒ ì •ê·œí™” ì ìš©
        reward = self.normalize_reward(reward)

        self.previous_portfolio_value = new_portfolio_value  

        # log_manager.logger.debug(f"Step: {self.current_step}, Action: {['Sell', 'Hold', 'Buy'][action]}, Reward: {reward}, Portfolio: {new_portfolio_value}, Shares Held: {self.shares_held}")

        return next_state, reward, done

if __name__ == "__main__":
    stock_data = np.random.randn(60, 5)
    env = StockTradingEnv(stock_data)
    state = env.reset()

    log_manager.logger.debug(f"ì´ˆê¸° ìƒíƒœ shape: {state.shape}")

    done = False
    step_count = 0

    while not done:
        next_state, reward, done, _ = env.step(2)  # ë§¤ìˆ˜ (Buy)
        step_count += 1
        log_manager.logger.debug(f"ğŸ”¹ Step: {step_count}, ë‹¤ìŒ ìƒíƒœ shape: {next_state.shape}, ë³´ìƒ: {reward}, ì¢…ë£Œ ì—¬ë¶€: {done}")

    log_manager.logger.debug("âœ… í™˜ê²½ ì¢…ë£Œ!")
