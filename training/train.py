import torch
import os
import sys
from torch.utils.tensorboard import SummaryWriter

current_file = os.path.abspath(__file__) 
project_root = os.path.abspath(os.path.join(current_file, "..", "..")) # í˜„ì¬ ë””ë ‰í† ë¦¬ì— ë”°ë¼ ì´ ë¶€ë¶„ ìˆ˜ì •
sys.path.append(project_root)

from manage import PathManager
path_manager = PathManager()

# ì›í•˜ëŠ” ê²½ë¡œ ì¶”ê°€
sys.path.append(path_manager.get_path("config"))
sys.path.append(path_manager.get_path("logs"))
sys.path.append(path_manager.get_path("env"))
sys.path.append(path_manager.get_path("models"))
sys.path.append(path_manager.get_path("agents"))
sys.path.append(path_manager.get_path("data"))

try:
    from logs import log_manager
    from config import config_manager
    from env.stock_env import StockTradingEnv
    from models.actor_network import ActorNetwork
    from models.critic_network import CriticNetwork
    from agents.actor_critic_agent import ActorCriticAgent
    from data.data_loader import load_stock_data
    from config import config_manager
except Exception as e:
    print(f"ì„í¬íŠ¸ ì‹¤íŒ¨: {e}")

class TrainingManager:
    """ í•™ìŠµëœ ëª¨ë¸ì„ ì €ì¥ ë° ê´€ë¦¬í•˜ëŠ” í´ë˜ìŠ¤ """
    def __init__(self, directory=None, filename="ppo_stock_trader.pth", checkpoint_filename="ppo_checkpoint.pth"):
        """
        TrainingManager ì´ˆê¸°í™”

        Args:
            directory (str, optional): ëª¨ë¸ ì €ì¥ ë””ë ‰í† ë¦¬. ê¸°ë³¸ê°’ì€ í”„ë¡œì íŠ¸ ë£¨íŠ¸ ë””ë ‰í† ë¦¬ì—ì„œ `output` í´ë”.
            filename (str): ì €ì¥í•  ëª¨ë¸ íŒŒì¼ ì´ë¦„.
            checkpoint_filename (str): ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ ì´ë¦„.
        """
        if not hasattr(self, 'initialized'):  # ì¸ìŠ¤í„´ìŠ¤ê°€ ì´ˆê¸°í™”ë˜ì—ˆëŠ”ì§€ í™•ì¸
            default_directory = os.path.join(os.path.dirname(os.path.abspath(__file__)), "..", "output")
            self.directory = directory or default_directory  # âœ… ì‚¬ìš©ìê°€ ì§€ì •í•œ ê²½ë¡œê°€ ì—†ìœ¼ë©´ ê¸°ë³¸ê°’ ì‚¬ìš©
            self.filename = filename
            self.checkpoint_filename = checkpoint_filename
            self.save_path = os.path.join(self.directory, self.filename)
            self.checkpoint_path = os.path.join(self.directory, self.checkpoint_filename)
            self.epsilon = config_manager.get_epsilon()
            log_manager.logger.debug(f"âœ… ëª¨ë¸ ì €ì¥ ê²½ë¡œ: {self.save_path}")

            os.makedirs(self.directory, exist_ok=True)  # í´ë”ê°€ ì—†ìœ¼ë©´ ìë™ ìƒì„±
            self.initialized = True

    def save_model(self, model, episode=None):
        """
        ëª¨ë¸ ê°€ì¤‘ì¹˜ë¥¼ ì €ì¥í•˜ëŠ” í•¨ìˆ˜ (ì „ì²´ ëª¨ë¸ì´ ì•„ë‹ˆë¼ ê°€ì¤‘ì¹˜ë§Œ ì €ì¥)

        Args:
            model (torch.nn.Module): ì €ì¥í•  ëª¨ë¸
            episode (int, optional): ì—í”¼ì†Œë“œ ë²ˆí˜¸ë¥¼ í¬í•¨í•˜ì—¬ ì €ì¥ (ê¸°ë³¸ê°’: None)
        """
        if episode is not None:
            filename = f"ppo_stock_trader_episode_{episode}.pth"  # âœ… ì—í”¼ì†Œë“œ ë²ˆí˜¸ í¬í•¨
        else:
            filename = self.filename

        save_path = os.path.join(self.directory, filename)

        try:
            torch.save(model.state_dict(), save_path)  # ğŸ”¥ ê°€ì¤‘ì¹˜ë§Œ ì €ì¥
            log_manager.logger.info(f"âœ… ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {save_path}")
        except Exception as e:
            log_manager.logger.error(f"âŒ ëª¨ë¸ ì €ì¥ ì‹¤íŒ¨: {e}")

    def save_checkpoint(self, actor, critic, optimizer_actor, optimizer_critic, episode):
        """
        ì²´í¬í¬ì¸íŠ¸ë¥¼ ì €ì¥í•˜ëŠ” í•¨ìˆ˜ (Actor-Critic)

        Args:
            actor (torch.nn.Module): Actor ëª¨ë¸
            critic (torch.nn.Module): Critic ëª¨ë¸
            optimizer_actor (torch.optim.Optimizer): Actor ì˜µí‹°ë§ˆì´ì €
            optimizer_critic (torch.optim.Optimizer): Critic ì˜µí‹°ë§ˆì´ì €
            episode (int): ì €ì¥í•  ì‹œì ì˜ ì—í”¼ì†Œë“œ ë²ˆí˜¸
        """
        checkpoint = {
            'actor_state_dict': actor.state_dict(),
            'critic_state_dict': critic.state_dict(),
            'optimizer_actor': optimizer_actor.state_dict(),
            'optimizer_critic': optimizer_critic.state_dict(),
            'episode': episode,
            'epsilon': self.epsilon
        }
        torch.save(checkpoint, self.checkpoint_path)
        log_manager.logger.info(f"âœ… ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ì™„ë£Œ: {self.checkpoint_path} (Episode {episode}, epsilon: {self.epsilon:.6f})")

    def load_checkpoint(self, actor, critic, optimizer_actor, optimizer_critic, agent=None):
        """
        ì²´í¬í¬ì¸íŠ¸ë¥¼ ë¡œë“œí•˜ëŠ” í•¨ìˆ˜

        Args:
            actor (torch.nn.Module): Actor ëª¨ë¸
            critic (torch.nn.Module): Critic ëª¨ë¸
            optimizer_actor (torch.optim.Optimizer): Actor ì˜µí‹°ë§ˆì´ì €
            optimizer_critic (torch.optim.Optimizer): Critic ì˜µí‹°ë§ˆì´ì €
            agent (ActorCriticAgent, optional): ì—ì´ì „íŠ¸ ì¸ìŠ¤í„´ìŠ¤ (epsilon ë³µì›ìš©)

        Returns:
            int: ë§ˆì§€ë§‰ ì €ì¥ëœ ì—í”¼ì†Œë“œ ë²ˆí˜¸. ì²´í¬í¬ì¸íŠ¸ê°€ ì—†ìœ¼ë©´ 0 ë°˜í™˜.
        """
        if os.path.exists(self.checkpoint_path):
            checkpoint = torch.load(self.checkpoint_path)
            actor.load_state_dict(checkpoint['actor_state_dict'])
            critic.load_state_dict(checkpoint['critic_state_dict'])
            optimizer_actor.load_state_dict(checkpoint['optimizer_actor'])
            optimizer_critic.load_state_dict(checkpoint['optimizer_critic'])
            episode = checkpoint['episode']
            self.epsilon = checkpoint.get('epsilon', self.epsilon)

            if agent is not None:
                agent.epsilon = self.epsilon  # âœ… PPOAgentì˜ epsilonë„ ì—…ë°ì´íŠ¸

            return episode
        else:
            log_manager.logger.info("âš ï¸ ì²´í¬í¬ì¸íŠ¸ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ìƒˆë¡œìš´ í•™ìŠµì„ ì‹œì‘í•©ë‹ˆë‹¤.")
            return 0

def train_agent(env, agent, episodes, training_manager):
    """ PPO ì—ì´ì „íŠ¸ë¥¼ í•™ìŠµì‹œí‚¤ëŠ” í•¨ìˆ˜ 
    
    Args:
        env (StockTradingEnv): ì£¼ì‹ ê±°ë˜ í™˜ê²½ ì¸ìŠ¤í„´ìŠ¤
        agent (ActorCriticAgent): í•™ìŠµí•  ì—ì´ì „íŠ¸
        episodes (int): ì´ í•™ìŠµ ì—í”¼ì†Œë“œ ìˆ˜
        training_manager (TrainingManager): ëª¨ë¸ ì €ì¥ ë° ì²´í¬í¬ì¸íŠ¸ ê´€ë¦¬ í´ë˜ìŠ¤
    """
    log_manager.logger.info(f"ğŸ¯ í•™ìŠµ ì‹œì‘")

    # ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ (ì´ì „ í•™ìŠµ ê¸°ë¡ì´ ìˆìœ¼ë©´ ì´ì–´ì„œ ì‹œì‘)
    start_episode = training_manager.load_checkpoint(agent.actor, agent.critic, agent.optimizer_actor, agent.optimizer_critic, agent)
    best_reward = float('-inf')  # ìµœê³  ë¦¬ì›Œë“œ ê¸°ë¡ ì´ˆê¸°í™”

    for episode in range(start_episode, episodes):
        state = env.reset()
        memory = []
        total_reward = 0

        for t in range(len(env.stock_data) - config_manager.get_observation_window()):
            # balance = env.balance
            # shares_held = env.shares_held
            # current_price = env.stock_data[env.current_step, 0]

            # âœ… PPOAgentì—ê²Œ í™˜ê²½ ì •ë³´ë¥¼ ì „ë‹¬í•˜ì—¬ ì•¡ì…˜ ì„ íƒ
            action, log_prob, value = agent.select_action(state)
            next_state, reward, done = env.step(action)
            memory.append((state, action, reward, log_prob, value))  # ë©”ëª¨ë¦¬ì— ì‹¤ì œ ì•¡ì…˜ ì €ì¥
            state = next_state
            total_reward += reward

            if done or len(memory) >= agent.batch_size:
                agent.update(memory)  # PPO ì—…ë°ì´íŠ¸ ìˆ˜í–‰
                memory = []  # ë°°ì¹˜ í•™ìŠµ í›„ ë©”ëª¨ë¦¬ ì´ˆê¸°í™”

        final_portfolio_value = env.balance + (env.shares_held * env.stock_data[env.current_step, 0])
        log_manager.logger.info(f"Episode {episode+1}/{episodes}, Total Reward: {total_reward}, final_portfolio_value: {final_portfolio_value:.2f}")

        # ë§¤ 100ë²ˆì§¸ ì—í”¼ì†Œë“œë§ˆë‹¤ ëª¨ë¸ê³¼ ì²´í¬í¬ì¸íŠ¸ ì €ì¥
        if (episode + 1) % 50 == 0:
            training_manager.save_model(agent.actor, episode=(episode + 1))
            training_manager.save_checkpoint(agent.actor, agent.critic, agent.optimizer_actor, agent.optimizer_critic, episode + 1)  # ì²´í¬í¬ì¸íŠ¸ ì €ì¥
            log_manager.logger.info(f"âœ… ì²´í¬í¬ì¸íŠ¸ ë° ëª¨ë¸ ì €ì¥ ì™„ë£Œ (Episode {episode+1})")

         # í˜„ì¬ ì—í”¼ì†Œë“œì˜ ë³´ìƒì´ ìµœê³  ë³´ìƒ(best_reward)ë³´ë‹¤ ë†’ì„ ê²½ìš° ì €ì¥
        if total_reward > best_reward:
            best_reward = total_reward  # ìµœê³  ë¦¬ì›Œë“œ ê°±ì‹ 
            training_manager.save_model(agent.actor, episode=(episode + 1))
            training_manager.save_checkpoint(agent.actor, agent.critic, agent.optimizer_actor, agent.optimizer_critic, episode + 1)  # ì²´í¬í¬ì¸íŠ¸ ì €ì¥
            log_manager.logger.info(f"âœ… ìµœê³  ë¦¬ì›Œë“œ ê°±ì‹ ! ëª¨ë¸ ì €ì¥ ì™„ë£Œ (Episode {episode+1})")

    # ìµœì¢… í•™ìŠµ ì™„ë£Œ í›„ ëª¨ë¸ ì €ì¥
    training_manager.save_model(agent.actor)
    log_manager.logger.info(f"âœ… ìµœì¢… ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {training_manager.save_path}")

if __name__ == "__main__":
    # âœ… TrainingManager ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
    training_manager = TrainingManager()

    # âœ… ë°ì´í„° ë¡œë“œ
    data, input_dim = load_stock_data("data/csv/005930.KS_combined_train_data.csv")

    # âœ… í…ì„œë³´ë“œ writer ìƒì„±
    writer = SummaryWriter(log_dir="logs/training")

    # âœ… í™˜ê²½ ë° ëª¨ë¸ ìƒì„±
    env = StockTradingEnv(data, writer=writer)
    actor = ActorNetwork(input_dim=input_dim)
    critic = CriticNetwork(input_dim=input_dim)
    agent = ActorCriticAgent(actor, critic, writer=writer)

    # âœ… í•™ìŠµ ì‹œì‘
    train_agent(env, agent, episodes=config_manager.get_episodes(), training_manager=training_manager)

